{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug\n",
    "\n",
    "from datetime import datetime\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# If using model previously trained on training data, specify path\n",
    "TRAINED_MODEL_PATH = os.path.join(MODEL_DIR, \"\")\n",
    "\n",
    "TRAIN_PATH = '../data/stage1_train/'\n",
    "TEST_PATH = '../data/stage1_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train and test IDs\n",
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=13)\n",
    "test_ids = next(os.walk(TEST_PATH))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NucleiConfig(Config):\n",
    "    \"\"\"Configuration for training on the cell nuclei dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the nuclei dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"nuclei\"\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 300\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    VALIDATION_STEPS = 100\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 2  # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 2\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 320\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resizing\n",
    "    # Images are resized such that the small side is IMAGE_MIN_DIM and\n",
    "    # the long side is <= IMAGE_MAX_DIM. If both conditions can't be\n",
    "    # satisfied at the same time then IMAGE_MAX_DIM is enforced.\n",
    "    # Resizing modes:\n",
    "    #     none: No resizing\n",
    "    #     square: Pad with zeros to make it a square (MAX_DIM, MAX_DIM)\n",
    "    # TODO: currently, only 'square' mode is supported\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 1024\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([0., 0., 0.])\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 512\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 256\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 300\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimzer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "config = NucleiConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NucleiDataset(utils.Dataset):\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        image = imread(self.image_info[image_id]['path'])[:, :, :3]\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        image_path = info['path']\n",
    "        mask_path = image_path[:image_path.find('/images/')]\n",
    "        mask_dir = os.path.join(mask_path, 'masks')\n",
    "        mask_names = os.listdir(mask_dir)\n",
    "        count = len(mask_names)\n",
    "        mask = []\n",
    "        for i, el in enumerate(mask_names):\n",
    "            msk_path = os.path.join(mask_dir, el)\n",
    "            msk = imread(msk_path)\n",
    "            if np.sum(msk) == 0:\n",
    "                print('invalid mask')\n",
    "                continue\n",
    "            msk = msk.astype('float32')/255.\n",
    "            mask.append(msk)\n",
    "        mask = np.asarray(mask)\n",
    "        mask[mask > 0.] = 1.\n",
    "        mask = np.transpose(mask, (1,2,0))\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        count = mask.shape[2]\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            mask[:, :, i] = binary_fill_holes(mask[:, :, i])\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        class_ids = [self.class_names.index('nucleus') for s in range(count)]\n",
    "        class_ids = np.asarray(class_ids)\n",
    "        return mask, class_ids.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = NucleiDataset()\n",
    "dataset_val = NucleiDataset()\n",
    "dataset_test = NucleiDataset()\n",
    "\n",
    "dataset_train.add_class(\"stage1_train\", 1, \"nucleus\")\n",
    "dataset_val.add_class(\"stage1_train\", 1, \"nucleus\")\n",
    "dataset_test.add_class(\"stage1_train\", 1, \"nucleus\")\n",
    "\n",
    "# Get and resize train images and masks\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img_path = path + '/images/' + id_ + '.png'\n",
    "    dataset_train.add_image('stage1_train', id_, img_path)\n",
    "\n",
    "# Get and resize validation images and masks\n",
    "print('Getting and resizing validation images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(val_ids), total=len(val_ids)):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img_path = path + '/images/' + id_ + '.png'\n",
    "    dataset_val.add_image('stage1_train', id_, img_path)\n",
    "\n",
    "# Get and resize test images\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = TEST_PATH + id_\n",
    "    img_path = path + '/images/' + id_ + '.png'\n",
    "    dataset_test.add_image('stage1_test', id_, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "dataset_train.prepare()\n",
    "dataset_val.prepare()\n",
    "dataset_test.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print(mask.shape, image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pretrained weights\n",
    "\n",
    "coco_weights = True\n",
    "if coco_weights:\n",
    "\t# Load weights trained on COCO dataset\n",
    "\tmodel.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "else:\n",
    "    # Load weights trained from previous training cycle\n",
    "    #model.load_weights(TRAINED_MODEL_PATH)\n",
    "    model_path = model.find_last()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data augmentation if using augmentation\n",
    "augmentation = imgaug.augmenters.Sometimes(0.5,\n",
    "    imgaug.augmenters.Affine(\n",
    "        #translate_percent = {\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-45, 45),\n",
    "        shear=(-15, 15)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern or train \"all\" layers.\n",
    "\n",
    "# Include \"augmentation=augmentation\" parameter if applying training data augmentation\n",
    "train_heads = False\n",
    "\n",
    "if train_heads:\n",
    "    model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=10,\n",
    "            layers=\"heads\")\n",
    "else:\n",
    "    model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=10,\n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights manually for convenience\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_resnet101_10e_grid21.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions - Segment and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(NucleiConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "inference_config.display()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\",\n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "#model_path = os.path.join(ROOT_DIR, \"mask_rcnn_nuclei_full.h5\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict masks on test images\n",
    "\n",
    "raw_predictions = []\n",
    "for test_id in dataset_test.image_ids:\n",
    "    test_image1 = dataset_test.load_image(test_id)\n",
    "    pred = model.detect([test_image1], verbose=0)\n",
    "    pred = pred[0]\n",
    "    sc = pred['scores']\n",
    "    pred = pred['masks']\n",
    "    raw_predictions.append((pred, sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run-length encoding borrowed from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode predictions into RLE.\n",
    "# Borrowed from https://github.com/neptune-ml/open-solution-data-science-bowl-2018/blob/mask_rcnn_notebook/main.ipynb\n",
    "def numpy2encoding_no_overlap(predicts, img_name, scores):\n",
    "    sum_predicts = np.sum(predicts, axis=2)\n",
    "    rows, cols = np.where(sum_predicts>=2)\n",
    "\n",
    "    for i in zip(rows, cols):\n",
    "        instance_indicies = np.where(np.any(predicts[i[0],i[1],:]))[0]\n",
    "        highest = instance_indicies[0]\n",
    "        predicts[i[0],i[1],:] = predicts[i[0],i[1],:]*0\n",
    "        predicts[i[0],i[1],highest] = 1\n",
    "\n",
    "    ImageId = []\n",
    "    EncodedPixels = []\n",
    "    print(predicts.shape)\n",
    "    for i in range(predicts.shape[2]):\n",
    "        rle = rle_encoding(predicts[:,:,i])\n",
    "        if len(rle)>0:\n",
    "            ImageId.append(img_name)\n",
    "            EncodedPixels.append(rle)\n",
    "    return ImageId, EncodedPixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_test_ids = []\n",
    "rles = []\n",
    "for id, raw_pred in zip(test_ids, raw_predictions):\n",
    "    ids, rle = numpy2encoding_no_overlap(raw_pred[0], id, raw_pred[1])\n",
    "    new_test_ids += ids\n",
    "    rles += rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create submission DataFrame and csv file\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "submission_file = 'mask-rcnn_resnet101_10e_grid21.csv'\n",
    "df = pd.DataFrame({ 'ImageId' : new_test_ids , 'EncodedPixels' : rles})\n",
    "df.to_csv('output/' + submission_file, index=False, columns=['ImageId', 'EncodedPixels'])\n",
    "\n",
    "print('Submission file saved at:', ROOT_DIR + '/output/' + submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
